{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read args\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-d\", \"--data_folder\", type=str, default=\"ehr\", \n",
    "    help=\"data folder name\")\n",
    "parser.add_argument(\"-a\", \"--aggregator\", type=str, default=\"transformer\", \n",
    "    help=\"aggregator name\", choices=['conflation', 'sanity_check', 'sanity_check_transformer', 'transformer'])\n",
    "parser.add_argument(\"-r\", \"--resume\", type=str, default = \"kmembert-base\", \n",
    "    help=\"result folder in which the saved checkpoint will be reused\")\n",
    "parser.add_argument(\"-e\", \"--epochs\", type=int, default=2, \n",
    "    help=\"number of epochs\")\n",
    "parser.add_argument(\"-nr\", \"--nrows\", type=int, default=None, \n",
    "    help=\"maximum number of samples for training and validation\")\n",
    "parser.add_argument(\"-k\", \"--print_every_k_batch\", type=int, default=1, \n",
    "    help=\"prints training loss every k batch\")\n",
    "parser.add_argument(\"-dt\", \"--days_threshold\", type=int, default=365, \n",
    "    help=\"days threshold to convert into classification task\")\n",
    "parser.add_argument(\"-lr\", \"--learning_rate\", type=float, default=1e-4, \n",
    "    help=\"model learning rate\")\n",
    "parser.add_argument(\"-wg\", \"--weight_decay\", type=float, default=0, \n",
    "    help=\"the weight decay for L2 regularization\")\n",
    "parser.add_argument(\"-p\", \"--patience\", type=int, default=4, \n",
    "    help=\"number of decreasing accuracy epochs to stop the training\")\n",
    "parser.add_argument(\"-me\", \"--max_ehrs\", type=int, default=4, \n",
    "    help=\"maximum nusmber of ehrs to be used for multi ehrs prediction\")\n",
    "parser.add_argument(\"-nh\", \"--nhead\", type=int, default=8, \n",
    "    help=\"number of transformer heads\")\n",
    "parser.add_argument(\"-nl\", \"--num_layers\", type=int, default=4, \n",
    "    help=\"number of transformer layers\")\n",
    "parser.add_argument(\"-od\", \"--out_dim\", type=int, default=2, \n",
    "    help=\"transformer out_dim (1 regression or 2 density)\")\n",
    "parser.add_argument(\"-td\", \"--time_dim\", type=int, default=8, \n",
    "    help=\"transformer time_dim\")\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "from kmembert.utils import Config\n",
    "from kmembert.models import TransformerAggregator\n",
    "from kmembert.dataset import PredictionsDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "from kmembert.dataset import PredictionsDataset\n",
    "from kmembert.models import TransformerAggregator\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score, roc_curve, r2_score, mean_squared_error\n",
    "from kmembert.utils import pretty_time, printc, create_session, save_json, get_label_threshold, get_error, time_survival_to_label, collate_fn\n",
    "from kmembert.utils import create_session, get_label_threshold, collate_fn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "import pandas as pd\n",
    "test = pd.read_csv(\"test_rs1_ind.csv\")\n",
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset, path_result, device, config = create_session(args)\n",
    "\n",
    "assert (768 + args.time_dim) % args.nhead == 0, f'd_model (i.e. 768 + time_dim) must be divisible by nhead. Found time_dim {args.time_dim} and nhead {args.nhead}'\n",
    "\n",
    "config.label_threshold = get_label_threshold(config, path_dataset)\n",
    "\n",
    "dataset = PredictionsDataset(path_dataset, config, output_hidden_states=True, device=device, train=False)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (*data, labels, raw) in enumerate(loader):\n",
    "    data = [[data[0][0]], torch.tensor(data[0][1], dtype=torch.float32)]\n",
    "    print(type(data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (*data, labels) in enumerate(loader):\n",
    "    print(data[1].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(np.array([22, 15,  0,  0]), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction to convert dates to time survival\n",
    "from datetime import datetime\n",
    "def strDate_to_days_cr(row, date_format = \"%Y-%m-%d\"):\n",
    "    '''\n",
    "        When there's no FLAG_DECES column in the df\n",
    "    '''\n",
    "    a = datetime.strptime(row['Date deces'], date_format)\n",
    "    b = datetime.strptime(row['Date cr'], date_format)\n",
    "    val = (a-b).days\n",
    "    return val\n",
    "def time_survival_to_label(time_survival, mean_time_survival=800):\n",
    "    \"\"\"\n",
    "    Transforms times of survival into uniform labels in ]0,1[\n",
    "    \"\"\"\n",
    "    return 1 - np.exp(-time_survival/mean_time_survival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data\\\\ehr\\\\test.csv\")\n",
    "test['Date cr'] = test['Date cr'].apply(lambda x: str(x))\n",
    "test['Date cr'] = test['Date cr'].apply(lambda x: '-'.join([x[:4], x[4:6], x[6:]]))\n",
    "test['Date deces'] = test['Date deces'].apply(lambda x: str(x))\n",
    "test['Date deces'] = test['Date deces'].apply(lambda x: '-'.join([x[:4], x[4:6], x[6:]]))\n",
    "test['time_surv_day'] = test.apply(strDate_to_days_cr, axis=1)\n",
    "test['time_surv'] = list(map(time_survival_to_label, test['time_surv_day']))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3017c399632bdad99ff7d78fdd8a75c4df8c74c804f4832978fc1a75e7107017"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('clb_env2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
