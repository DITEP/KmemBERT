{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertForSequenceClassification, CamembertTokenizer, pipeline, CamembertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"camembert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained(model_name)\n",
    "#cam = CamembertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\"J'ai passé une super journée\", \"Grosses galères aujourd'hui\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> J'ai passé une super journée</s>\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(tweet)\n",
    "tokenizer.encode(tweet)\n",
    "tokenizer.decode(tokenizer.encode(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis', model=model_name, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.5017223358154297}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"Il fait très beau !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForMaskedLM were not initialized from the model checkpoint at camembert-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': '<s> Le camembert est délicieux :)</s>',\n",
       "  'score': 0.49091020226478577,\n",
       "  'token': 7200,\n",
       "  'token_str': '▁délicieux'},\n",
       " {'sequence': '<s> Le camembert est excellent :)</s>',\n",
       "  'score': 0.10556937754154205,\n",
       "  'token': 2183,\n",
       "  'token_str': '▁excellent'},\n",
       " {'sequence': '<s> Le camembert est succulent :)</s>',\n",
       "  'score': 0.034533143043518066,\n",
       "  'token': 26202,\n",
       "  'token_str': '▁succulent'},\n",
       " {'sequence': '<s> Le camembert est meilleur :)</s>',\n",
       "  'score': 0.03303135931491852,\n",
       "  'token': 528,\n",
       "  'token_str': '▁meilleur'},\n",
       " {'sequence': '<s> Le camembert est parfait :)</s>',\n",
       "  'score': 0.030076511204242706,\n",
       "  'token': 1654,\n",
       "  'token_str': '▁parfait'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camembert_fill_mask  = pipeline(\"fill-mask\", model=model_name, tokenizer=tokenizer)\n",
    "camembert_fill_mask(\"Le camembert est <mask> :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': \"<s> J'aime les yeux bleus!</s>\",\n",
       "  'score': 0.0990937277674675,\n",
       "  'token': 605,\n",
       "  'token_str': '▁yeux'},\n",
       " {'sequence': \"<s> J'aime les cheveux bleus!</s>\",\n",
       "  'score': 0.04541785269975662,\n",
       "  'token': 1277,\n",
       "  'token_str': '▁cheveux'},\n",
       " {'sequence': \"<s> J'aime les chats bleus!</s>\",\n",
       "  'score': 0.03782883286476135,\n",
       "  'token': 6289,\n",
       "  'token_str': '▁chats'},\n",
       " {'sequence': \"<s> J'aime les poissons bleus!</s>\",\n",
       "  'score': 0.035177621990442276,\n",
       "  'token': 4831,\n",
       "  'token_str': '▁poissons'},\n",
       " {'sequence': \"<s> J'aime les oiseaux bleus!</s>\",\n",
       "  'score': 0.026914037764072418,\n",
       "  'token': 5709,\n",
       "  'token_str': '▁oiseaux'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camembert_fill_mask(\"J'aime les <mask> bleus!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(tweets, return_tensors='pt', padding=True, truncation=True)\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = camembert(input_ids, attention_mask=attention_mask, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.7065, grad_fn=<NllLossBackward>), logits=tensor([[ 0.0557, -0.1002],\n",
       "        [ 0.0349, -0.0768]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(['onco'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁un', '▁test', '▁', 'astrologie', 'onco', '▁', 'logie', '▁', 'logie', '▁regardé', 'es', '▁t', 'roma', 'to']\n",
      "[23, 2006, 21, 15174, 32005, 21, 4736, 21, 4736, 8031, 80, 271, 12295, 891]\n"
     ]
    }
   ],
   "source": [
    "text = 'un test astrologie oncologie logie regardées tromato'\n",
    "print(tokenizer.tokenize(text))\n",
    "print(tokenizer.encode(text)[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<s>NOTUSED': 0,\n",
       " '<pad>': 1,\n",
       " '</s>NOTUSED': 2,\n",
       " '<unk>': 4,\n",
       " '<s>': 5,\n",
       " '</s>': 6,\n",
       " ',': 7,\n",
       " '▁de': 8,\n",
       " '.': 9,\n",
       " 's': 10,\n",
       " \"'\": 11,\n",
       " '’': 12,\n",
       " '▁la': 13,\n",
       " '▁et': 14,\n",
       " '▁à': 15,\n",
       " '▁le': 16,\n",
       " '▁l': 17,\n",
       " '▁d': 18,\n",
       " '▁les': 19,\n",
       " '▁des': 20,\n",
       " '▁': 21,\n",
       " '▁en': 22,\n",
       " '▁un': 23,\n",
       " '▁pour': 24,\n",
       " '▁du': 25,\n",
       " '-': 26,\n",
       " '▁que': 27,\n",
       " '▁une': 28,\n",
       " '▁dans': 29,\n",
       " '▁est': 30,\n",
       " '▁qui': 31,\n",
       " '▁sur': 32,\n",
       " '▁a': 33,\n",
       " '▁pas': 34,\n",
       " 'e': 35,\n",
       " '▁au': 36,\n",
       " '▁par': 37,\n",
       " '▁(': 38,\n",
       " '▁vous': 39,\n",
       " '▁plus': 40,\n",
       " 'est': 41,\n",
       " '▁avec': 42,\n",
       " '▁:': 43,\n",
       " '▁ce': 44,\n",
       " '▁ne': 45,\n",
       " '▁qu': 46,\n",
       " '▁ou': 47,\n",
       " '▁se': 48,\n",
       " '▁n': 49,\n",
       " '▁je': 50,\n",
       " '▁il': 51,\n",
       " '▁s': 52,\n",
       " ')': 53,\n",
       " '▁Le': 54,\n",
       " 'a': 55,\n",
       " '▁sont': 56,\n",
       " '...': 57,\n",
       " '▁son': 58,\n",
       " 'un': 59,\n",
       " '▁c': 60,\n",
       " '▁La': 61,\n",
       " 'il': 62,\n",
       " '▁nous': 63,\n",
       " '▁«': 64,\n",
       " '▁mais': 65,\n",
       " '▁tout': 66,\n",
       " '▁-': 67,\n",
       " '▁aux': 68,\n",
       " '▁Il': 69,\n",
       " 'une': 70,\n",
       " '▁L': 71,\n",
       " '▁bien': 72,\n",
       " 'ai': 73,\n",
       " '▁Les': 74,\n",
       " '▁votre': 75,\n",
       " '▁j': 76,\n",
       " '▁sa': 77,\n",
       " '▁cette': 78,\n",
       " '▁comme': 79,\n",
       " 'es': 80,\n",
       " 'r': 81,\n",
       " '▁fait': 82,\n",
       " '▁!': 83,\n",
       " '▁C': 84,\n",
       " '▁faire': 85,\n",
       " '▁si': 86,\n",
       " '▁\"': 87,\n",
       " 'on': 88,\n",
       " '▁ses': 89,\n",
       " 'en': 90,\n",
       " '▁on': 91,\n",
       " ':': 92,\n",
       " '▁même': 93,\n",
       " '▁»': 94,\n",
       " '▁très': 95,\n",
       " '▁ont': 96,\n",
       " '▁leur': 97,\n",
       " '▁être': 98,\n",
       " '▁aussi': 99,\n",
       " '▁Je': 100,\n",
       " '▁été': 101,\n",
       " '▁y': 102,\n",
       " '▁me': 103,\n",
       " '▁peut': 104,\n",
       " 'y': 105,\n",
       " '▁?': 106,\n",
       " '▁En': 107,\n",
       " 'er': 108,\n",
       " '▁elle': 109,\n",
       " 't': 110,\n",
       " '▁lui': 111,\n",
       " '▁sans': 112,\n",
       " 'nt': 113,\n",
       " '▁A': 114,\n",
       " '▁m': 115,\n",
       " '▁deux': 116,\n",
       " '▁tous': 117,\n",
       " '▁2': 118,\n",
       " '▁ces': 119,\n",
       " ').': 120,\n",
       " '▁J': 121,\n",
       " '/': 122,\n",
       " '▁Pour': 123,\n",
       " '▁1': 124,\n",
       " '▁temps': 125,\n",
       " '▁peu': 126,\n",
       " '▁notre': 127,\n",
       " '▁entre': 128,\n",
       " '▁mon': 129,\n",
       " '\"': 130,\n",
       " 'ment': 131,\n",
       " '▁site': 132,\n",
       " 'h': 133,\n",
       " '▁ans': 134,\n",
       " '▁3': 135,\n",
       " '▁ça': 136,\n",
       " '▁De': 137,\n",
       " 'z': 138,\n",
       " '▁Et': 139,\n",
       " '▁vos': 140,\n",
       " 'é': 141,\n",
       " '),': 142,\n",
       " '▁encore': 143,\n",
       " 'elle': 144,\n",
       " '▁donc': 145,\n",
       " '▁suis': 146,\n",
       " '▁où': 147,\n",
       " '▁Ce': 148,\n",
       " '▁était': 149,\n",
       " '▁tu': 150,\n",
       " '▁fois': 151,\n",
       " '!': 152,\n",
       " '▁Un': 153,\n",
       " ';': 154,\n",
       " '▁ma': 155,\n",
       " '▁S': 156,\n",
       " '▁vie': 157,\n",
       " '▁Vous': 158,\n",
       " '▁Mais': 159,\n",
       " '▁D': 160,\n",
       " '▁sous': 161,\n",
       " '▁rencontre': 162,\n",
       " '▁ainsi': 163,\n",
       " '▁monde': 164,\n",
       " '▁non': 165,\n",
       " '▁nos': 166,\n",
       " '▁;': 167,\n",
       " '▁Si': 168,\n",
       " 'à': 169,\n",
       " '▁Nous': 170,\n",
       " '▁avait': 171,\n",
       " 'ant': 172,\n",
       " '▁car': 173,\n",
       " '▁dont': 174,\n",
       " '▁moins': 175,\n",
       " '▁depuis': 176,\n",
       " 'être': 177,\n",
       " '▁avant': 178,\n",
       " '▁toujours': 179,\n",
       " '▁Une': 180,\n",
       " '▁4': 181,\n",
       " '▁après': 182,\n",
       " '▁alors': 183,\n",
       " '▁France': 184,\n",
       " 'le': 185,\n",
       " '▁...': 186,\n",
       " '▁leurs': 187,\n",
       " '▁M': 188,\n",
       " 'o': 189,\n",
       " '▁avoir': 190,\n",
       " '▁soit': 191,\n",
       " '▁contre': 192,\n",
       " '▁quelques': 193,\n",
       " '▁toute': 194,\n",
       " '▁Elle': 195,\n",
       " '▁–': 196,\n",
       " '?': 197,\n",
       " '▁va': 198,\n",
       " 'ait': 199,\n",
       " '▁également': 200,\n",
       " '▁On': 201,\n",
       " '▁moi': 202,\n",
       " '▁cas': 203,\n",
       " 'd': 204,\n",
       " '▁5': 205,\n",
       " '▁quand': 206,\n",
       " '▁cela': 207,\n",
       " '▁toutes': 208,\n",
       " '▁jour': 209,\n",
       " '▁sera': 210,\n",
       " '▁Dans': 211,\n",
       " '▁bon': 212,\n",
       " '▁faut': 213,\n",
       " '▁autres': 214,\n",
       " 'm': 215,\n",
       " 'c': 216,\n",
       " '▁beaucoup': 217,\n",
       " '▁place': 218,\n",
       " 'l': 219,\n",
       " '▁ils': 220,\n",
       " '▁grand': 221,\n",
       " '▁chez': 222,\n",
       " '▁voir': 223,\n",
       " '▁vers': 224,\n",
       " '▁travail': 225,\n",
       " '▁petit': 226,\n",
       " '▁dit': 227,\n",
       " 'C': 228,\n",
       " 'S': 229,\n",
       " 'était': 230,\n",
       " 'et': 231,\n",
       " '▁Cette': 232,\n",
       " 'i': 233,\n",
       " 'de': 234,\n",
       " '▁déjà': 235,\n",
       " 'in': 236,\n",
       " '▁trop': 237,\n",
       " '▁autre': 238,\n",
       " '▁10': 239,\n",
       " 'ils': 240,\n",
       " '▁là': 241,\n",
       " '▁personnes': 242,\n",
       " 'A': 243,\n",
       " 'ci': 244,\n",
       " '▁partie': 245,\n",
       " '▁premier': 246,\n",
       " '▁plusieurs': 247,\n",
       " '▁dire': 248,\n",
       " '▁mes': 249,\n",
       " '▁mois': 250,\n",
       " '▁chaque': 251,\n",
       " 'eau': 252,\n",
       " '2': 253,\n",
       " '▁rien': 254,\n",
       " 'n': 255,\n",
       " '▁pays': 256,\n",
       " '▁jusqu': 257,\n",
       " '▁prix': 258,\n",
       " '▁fin': 259,\n",
       " '▁6': 260,\n",
       " '▁B': 261,\n",
       " '▁moment': 262,\n",
       " 'ont': 263,\n",
       " '▁puis': 264,\n",
       " 'hui': 265,\n",
       " 'autres': 266,\n",
       " 'ez': 267,\n",
       " '▁/': 268,\n",
       " '▁maison': 269,\n",
       " '▁femme': 270,\n",
       " '▁t': 271,\n",
       " '▁première': 272,\n",
       " 'ons': 273,\n",
       " '▁jours': 274,\n",
       " '▁P': 275,\n",
       " 'au': 276,\n",
       " '▁Au': 277,\n",
       " '▁N': 278,\n",
       " '▁doit': 279,\n",
       " '▁cet': 280,\n",
       " '▁nouveau': 281,\n",
       " '▁suite': 282,\n",
       " '▁jamais': 283,\n",
       " '▁ligne': 284,\n",
       " '▁ville': 285,\n",
       " 'p': 286,\n",
       " '▁compte': 287,\n",
       " '▁permet': 288,\n",
       " '▁afin': 289,\n",
       " 'x': 290,\n",
       " 'ce': 291,\n",
       " '▁part': 292,\n",
       " '▁grande': 293,\n",
       " '▁pouvez': 294,\n",
       " '▁mal': 295,\n",
       " '▁avons': 296,\n",
       " 'ur': 297,\n",
       " '▁lors': 298,\n",
       " '▁point': 299,\n",
       " '▁Paris': 300,\n",
       " '▁18': 301,\n",
       " '▁vraiment': 302,\n",
       " '▁trois': 303,\n",
       " '▁nouvelle': 304,\n",
       " 'ra': 305,\n",
       " '▁|': 306,\n",
       " '▁cours': 307,\n",
       " 'ou': 308,\n",
       " '▁T': 309,\n",
       " 'com': 310,\n",
       " '▁»,': 311,\n",
       " 'it': 312,\n",
       " '▁prendre': 313,\n",
       " '▁personne': 314,\n",
       " 'vous': 315,\n",
       " '▁peuvent': 316,\n",
       " '▁bonne': 317,\n",
       " '▁années': 318,\n",
       " '▁savoir': 319,\n",
       " '▁ceux': 320,\n",
       " 'la': 321,\n",
       " '▁lieu': 322,\n",
       " '▁ici': 323,\n",
       " 'ne': 324,\n",
       " '▁20': 325,\n",
       " 'ent': 326,\n",
       " '▁projet': 327,\n",
       " '▁mettre': 328,\n",
       " '▁enfants': 329,\n",
       " '▁celui': 330,\n",
       " '▁eu': 331,\n",
       " '▁recherche': 332,\n",
       " '▁7': 333,\n",
       " '▁mieux': 334,\n",
       " '▁qualité': 335,\n",
       " '▁produits': 336,\n",
       " '▁chose': 337,\n",
       " '▁15': 338,\n",
       " '▁pendant': 339,\n",
       " '▁effet': 340,\n",
       " 'al': 341,\n",
       " 'D': 342,\n",
       " '▁re': 343,\n",
       " '▁8': 344,\n",
       " '▁R': 345,\n",
       " 're': 346,\n",
       " '▁droit': 347,\n",
       " '▁dernier': 348,\n",
       " '▁avez': 349,\n",
       " '▁partir': 350,\n",
       " '▁pouvoir': 351,\n",
       " '▁te': 352,\n",
       " '▁reste': 353,\n",
       " '▁petite': 354,\n",
       " '▁souvent': 355,\n",
       " '▁trouver': 356,\n",
       " '▁jeu': 357,\n",
       " '▁F': 358,\n",
       " '▁niveau': 359,\n",
       " '▁E': 360,\n",
       " '▁G': 361,\n",
       " 'f': 362,\n",
       " '▁Des': 363,\n",
       " 'an': 364,\n",
       " '▁nombre': 365,\n",
       " '▁service': 366,\n",
       " '1': 367,\n",
       " '▁groupe': 368,\n",
       " 'autre': 369,\n",
       " 'L': 370,\n",
       " '▁».': 371,\n",
       " '▁Par': 372,\n",
       " '▁nom': 373,\n",
       " ']': 374,\n",
       " '▁mise': 375,\n",
       " '▁tant': 376,\n",
       " '▁Saint': 377,\n",
       " '▁in': 378,\n",
       " '▁plan': 379,\n",
       " '▁vu': 380,\n",
       " '▁surtout': 381,\n",
       " '▁ni': 382,\n",
       " 'g': 383,\n",
       " '▁possible': 384,\n",
       " 'B': 385,\n",
       " '▁celle': 386,\n",
       " '▁p': 387,\n",
       " '\\xad': 388,\n",
       " '▁femmes': 389,\n",
       " '▁juste': 390,\n",
       " '&': 391,\n",
       " '▁coup': 392,\n",
       " '▁jeune': 393,\n",
       " '▁besoin': 394,\n",
       " '▁selon': 395,\n",
       " '▁trouve': 396,\n",
       " '▁question': 397,\n",
       " '▁parce': 398,\n",
       " 'E': 399,\n",
       " '▁demande': 400,\n",
       " '▁famille': 401,\n",
       " 'ing': 402,\n",
       " '▁[': 403,\n",
       " '▁comment': 404,\n",
       " '▁aujourd': 405,\n",
       " 'P': 406,\n",
       " '▁Après': 407,\n",
       " 'les': 408,\n",
       " '▁12': 409,\n",
       " '▁notamment': 410,\n",
       " '▁exemple': 411,\n",
       " 'T': 412,\n",
       " '(': 413,\n",
       " '▁choix': 414,\n",
       " '▁ton': 415,\n",
       " '▁données': 416,\n",
       " '▁30': 417,\n",
       " 'is': 418,\n",
       " '▁9': 419,\n",
       " '▁certains': 420,\n",
       " '▁homme': 421,\n",
       " 'ée': 422,\n",
       " '▁côté': 423,\n",
       " '▁assez': 424,\n",
       " '▁ré': 425,\n",
       " '▁société': 426,\n",
       " 'fr': 427,\n",
       " '▁seul': 428,\n",
       " '▁façon': 429,\n",
       " '▁français': 430,\n",
       " '▁forme': 431,\n",
       " '▁Avec': 432,\n",
       " '▁année': 433,\n",
       " '▁gratuit': 434,\n",
       " '▁grâce': 435,\n",
       " '▁Ils': 436,\n",
       " '▁sens': 437,\n",
       " '▁porte': 438,\n",
       " '▁système': 439,\n",
       " '▁services': 440,\n",
       " '▁propose': 441,\n",
       " 'b': 442,\n",
       " 'avoir': 443,\n",
       " '▁passer': 444,\n",
       " '▁simple': 445,\n",
       " '▁seulement': 446,\n",
       " '▁salle': 447,\n",
       " '▁serait': 448,\n",
       " 'F': 449,\n",
       " '▁tête': 450,\n",
       " 'or': 451,\n",
       " 'H': 452,\n",
       " '▁%': 453,\n",
       " '▁H': 454,\n",
       " '▁belle': 455,\n",
       " '▁pu': 456,\n",
       " '▁V': 457,\n",
       " '▁17': 458,\n",
       " '▁rapport': 459,\n",
       " '▁type': 460,\n",
       " '▁face': 461,\n",
       " '▁politique': 462,\n",
       " '3': 463,\n",
       " '▁sommes': 464,\n",
       " 'M': 465,\n",
       " '▁devant': 466,\n",
       " '▁mis': 467,\n",
       " '▁À': 468,\n",
       " '▁photos': 469,\n",
       " '▁Jean': 470,\n",
       " '▁rencontres': 471,\n",
       " 'ation': 472,\n",
       " '▁cadre': 473,\n",
       " '▁eux': 474,\n",
       " '▁16': 475,\n",
       " '▁14': 476,\n",
       " '▁vue': 477,\n",
       " '▁près': 478,\n",
       " '▁début': 479,\n",
       " 'ais': 480,\n",
       " '▁quelque': 481,\n",
       " '▁K': 482,\n",
       " 'aux': 483,\n",
       " '▁quoi': 484,\n",
       " '▁“': 485,\n",
       " '▁corps': 486,\n",
       " 'te': 487,\n",
       " 'aient': 488,\n",
       " '▁main': 489,\n",
       " '▁nombreux': 490,\n",
       " 'age': 491,\n",
       " '▁film': 492,\n",
       " '▁long': 493,\n",
       " '▁base': 494,\n",
       " '▁êtes': 495,\n",
       " 'k': 496,\n",
       " '▁gens': 497,\n",
       " '▁etc': 498,\n",
       " '▁développement': 499,\n",
       " '▁pense': 500,\n",
       " '▁produit': 501,\n",
       " '▁Du': 502,\n",
       " '▁Sa': 503,\n",
       " '▁font': 504,\n",
       " '▁plutôt': 505,\n",
       " '▁manière': 506,\n",
       " '▁passe': 507,\n",
       " '▁tour': 508,\n",
       " '▁donner': 509,\n",
       " '▁livre': 510,\n",
       " '▁heures': 511,\n",
       " 'même': 512,\n",
       " '▁formation': 513,\n",
       " '▁maintenant': 514,\n",
       " '▁Ces': 515,\n",
       " '▁cuisine': 516,\n",
       " '\",': 517,\n",
       " 'u': 518,\n",
       " '▁seront': 519,\n",
       " 'année': 520,\n",
       " 'ie': 521,\n",
       " '▁juin': 522,\n",
       " '▁pris': 523,\n",
       " 'avais': 524,\n",
       " '▁public': 525,\n",
       " 'v': 526,\n",
       " 'apos': 527,\n",
       " '▁meilleur': 528,\n",
       " '▁hommes': 529,\n",
       " '▁étaient': 530,\n",
       " 'us': 531,\n",
       " '▁11': 532,\n",
       " '▁centre': 533,\n",
       " '▁semaine': 534,\n",
       " '▁ta': 535,\n",
       " '▁fille': 536,\n",
       " '▁&': 537,\n",
       " '▁jeunes': 538,\n",
       " '▁raison': 539,\n",
       " '▁haut': 540,\n",
       " '▁choses': 541,\n",
       " '▁autour': 542,\n",
       " '▁Tout': 543,\n",
       " 'ème': 544,\n",
       " '▁Sur': 545,\n",
       " 'G': 546,\n",
       " '▁fut': 547,\n",
       " '▁sécurité': 548,\n",
       " 'histoire': 549,\n",
       " 'V': 550,\n",
       " '▁I': 551,\n",
       " '▁2018': 552,\n",
       " '▁journée': 553,\n",
       " '▁Comme': 554,\n",
       " '▁sais': 555,\n",
       " '▁donne': 556,\n",
       " '▁bois': 557,\n",
       " '▁sexe': 558,\n",
       " 'art': 559,\n",
       " '▁13': 560,\n",
       " '▁santé': 561,\n",
       " '▁passé': 562,\n",
       " '▁page': 563,\n",
       " '▁dès': 564,\n",
       " '▁titre': 565,\n",
       " 'és': 566,\n",
       " 'agit': 567,\n",
       " '▁Re': 568,\n",
       " '▁marché': 569,\n",
       " '▁dé': 570,\n",
       " '°': 571,\n",
       " '▁travers': 572,\n",
       " 'me': 573,\n",
       " '▁Alors': 574,\n",
       " '▁pourrait': 575,\n",
       " '▁dernière': 576,\n",
       " '▁problème': 577,\n",
       " '4': 578,\n",
       " '▁différents': 579,\n",
       " '▁membres': 580,\n",
       " '▁région': 581,\n",
       " '▁elles': 582,\n",
       " '▁loin': 583,\n",
       " '▁sujet': 584,\n",
       " 'O': 585,\n",
       " '▁mesure': 586,\n",
       " '▁marque': 587,\n",
       " '▁nouvelles': 588,\n",
       " '▁loi': 589,\n",
       " '▁pourquoi': 590,\n",
       " '▁découvrir': 591,\n",
       " '▁informations': 592,\n",
       " '▁plaisir': 593,\n",
       " '▁fonction': 594,\n",
       " '▁situation': 595,\n",
       " '▁sites': 596,\n",
       " '▁+': 597,\n",
       " '▁seule': 598,\n",
       " '▁e': 599,\n",
       " '▁vrai': 600,\n",
       " 'eur': 601,\n",
       " '▁gros': 602,\n",
       " 'j': 603,\n",
       " '▁veut': 604,\n",
       " '▁yeux': 605,\n",
       " '▁général': 606,\n",
       " 'N': 607,\n",
       " '▁chambre': 608,\n",
       " '▁minutes': 609,\n",
       " '▁parfois': 610,\n",
       " '▁petits': 611,\n",
       " 'ine': 612,\n",
       " '▁lorsque': 613,\n",
       " '▁>': 614,\n",
       " '»': 615,\n",
       " '▁h': 616,\n",
       " 'ailleurs': 617,\n",
       " '▁25': 618,\n",
       " '▁soir': 619,\n",
       " '▁vient': 620,\n",
       " '▁photo': 621,\n",
       " 'at': 622,\n",
       " '\".': 623,\n",
       " '▁000': 624,\n",
       " '▁cause': 625,\n",
       " '▁mort': 626,\n",
       " '▁propre': 627,\n",
       " '▁offre': 628,\n",
       " 'R': 629,\n",
       " '▁semble': 630,\n",
       " '▁aucun': 631,\n",
       " '▁aller': 632,\n",
       " 'air': 633,\n",
       " '▁ayant': 634,\n",
       " '▁but': 635,\n",
       " 'as': 636,\n",
       " '▁carte': 637,\n",
       " '▁toi': 638,\n",
       " '▁parler': 639,\n",
       " 'w': 640,\n",
       " '▁24': 641,\n",
       " '▁ci': 642,\n",
       " '▁conditions': 643,\n",
       " '▁programme': 644,\n",
       " '▁mode': 645,\n",
       " '▁vidéo': 646,\n",
       " 'el': 647,\n",
       " '▁couleur': 648,\n",
       " 'ensemble': 649,\n",
       " 'que': 650,\n",
       " 'able': 651,\n",
       " '▁septembre': 652,\n",
       " '▁19': 653,\n",
       " '▁saison': 654,\n",
       " 'J': 655,\n",
       " '▁nuit': 656,\n",
       " '://': 657,\n",
       " '▁plein': 658,\n",
       " '▁Tu': 659,\n",
       " 'aime': 660,\n",
       " '▁cher': 661,\n",
       " 'je': 662,\n",
       " '▁autant': 663,\n",
       " '▁aurait': 664,\n",
       " '▁retour': 665,\n",
       " '▁ensuite': 666,\n",
       " '▁mai': 667,\n",
       " '▁président': 668,\n",
       " '▁g': 669,\n",
       " '▁beau': 670,\n",
       " '▁genre': 671,\n",
       " '▁peux': 672,\n",
       " '▁série': 673,\n",
       " '▁an': 674,\n",
       " 'co': 675,\n",
       " '▁vais': 676,\n",
       " '▁étant': 677,\n",
       " '▁différentes': 678,\n",
       " '▁présente': 679,\n",
       " '▁musique': 680,\n",
       " '▁Dieu': 681,\n",
       " '▁•': 682,\n",
       " '▁Cela': 683,\n",
       " '▁bas': 684,\n",
       " '▁Son': 685,\n",
       " '▁novembre': 686,\n",
       " 'avait': 687,\n",
       " '▁charge': 688,\n",
       " '▁présent': 689,\n",
       " 'occasion': 690,\n",
       " '▁simplement': 691,\n",
       " '▁W': 692,\n",
       " '▁important': 693,\n",
       " '▁moyen': 694,\n",
       " '▁janvier': 695,\n",
       " '▁nature': 696,\n",
       " '▁mars': 697,\n",
       " 'homme': 698,\n",
       " '▁entreprises': 699,\n",
       " '▁quatre': 700,\n",
       " '▁dis': 701,\n",
       " '▁vente': 702,\n",
       " '▁travaux': 703,\n",
       " '▁nouveaux': 704,\n",
       " '▁sûr': 705,\n",
       " '▁pratique': 706,\n",
       " 'K': 707,\n",
       " '▁création': 708,\n",
       " '00': 709,\n",
       " '▁avis': 710,\n",
       " '▁aura': 711,\n",
       " '▁50': 712,\n",
       " '▁millions': 713,\n",
       " '▁jeux': 714,\n",
       " '▁vite': 715,\n",
       " '▁rendre': 716,\n",
       " '▁doute': 717,\n",
       " '▁gestion': 718,\n",
       " 'W': 719,\n",
       " '▁nombreuses': 720,\n",
       " '▁terre': 721,\n",
       " '▁prise': 722,\n",
       " 'ma': 723,\n",
       " '▁article': 724,\n",
       " '▁points': 725,\n",
       " '▁grands': 726,\n",
       " '▁21': 727,\n",
       " 'té': 728,\n",
       " '▁fond': 729,\n",
       " '▁ca': 730,\n",
       " '▁sein': 731,\n",
       " '▁octobre': 732,\n",
       " '▁Car': 733,\n",
       " 'elles': 734,\n",
       " '▁fort': 735,\n",
       " '▁rapidement': 736,\n",
       " '▁tard': 737,\n",
       " '▁histoire': 738,\n",
       " '▁créer': 739,\n",
       " '▁siècle': 740,\n",
       " '▁taille': 741,\n",
       " '▁juillet': 742,\n",
       " '▁enfin': 743,\n",
       " 'tu': 744,\n",
       " '▁laquelle': 745,\n",
       " '▁Pas': 746,\n",
       " '▁vivre': 747,\n",
       " '▁O': 748,\n",
       " '▁date': 749,\n",
       " '▁doivent': 750,\n",
       " 'ch': 751,\n",
       " '6': 752,\n",
       " '▁décembre': 753,\n",
       " '▁gouvernement': 754,\n",
       " '▁2017': 755,\n",
       " '▁questions': 756,\n",
       " '▁Merci': 757,\n",
       " '▁x': 758,\n",
       " '▁prend': 759,\n",
       " '▁ensemble': 760,\n",
       " 'aide': 761,\n",
       " '▁noir': 762,\n",
       " '▁matière': 763,\n",
       " 'ir': 764,\n",
       " 'I': 765,\n",
       " '▁cœur': 766,\n",
       " '▁frais': 767,\n",
       " '▁ai': 768,\n",
       " '▁Ma': 769,\n",
       " '▁particulier': 770,\n",
       " '▁aucune': 771,\n",
       " '▁Mon': 772,\n",
       " '▁oui': 773,\n",
       " '▁vont': 774,\n",
       " '▁guerre': 775,\n",
       " '▁modèle': 776,\n",
       " '▁liste': 777,\n",
       " 'na': 778,\n",
       " '▁100': 779,\n",
       " '▁version': 780,\n",
       " '▁française': 781,\n",
       " '▁période': 782,\n",
       " '▁clients': 783,\n",
       " '▁amis': 784,\n",
       " '▁Notre': 785,\n",
       " '▁of': 786,\n",
       " '▁certaines': 787,\n",
       " '▁terme': 788,\n",
       " '▁chacun': 789,\n",
       " '▁pré': 790,\n",
       " '▁traitement': 791,\n",
       " '▁envie': 792,\n",
       " 'ta': 793,\n",
       " '▁force': 794,\n",
       " '▁Depuis': 795,\n",
       " '▁quel': 796,\n",
       " 'entreprise': 797,\n",
       " '▁blog': 798,\n",
       " '▁Que': 799,\n",
       " '5': 800,\n",
       " 'équipe': 801,\n",
       " 'article': 802,\n",
       " 'abord': 803,\n",
       " '▁donné': 804,\n",
       " '▁voyage': 805,\n",
       " '▁mer': 806,\n",
       " '▁[...]': 807,\n",
       " '▁the': 808,\n",
       " 'ter': 809,\n",
       " '▁valeur': 810,\n",
       " '▁facile': 811,\n",
       " '▁risque': 812,\n",
       " '▁domaine': 813,\n",
       " '▁entreprise': 814,\n",
       " '▁équipe': 815,\n",
       " 'ed': 816,\n",
       " 'utilisation': 817,\n",
       " '▁lien': 818,\n",
       " '▁père': 819,\n",
       " '▁résultats': 820,\n",
       " '▁Ne': 821,\n",
       " '▁comprendre': 822,\n",
       " '▁matin': 823,\n",
       " 'man': 824,\n",
       " '▁activités': 825,\n",
       " '30': 826,\n",
       " '▁compris': 827,\n",
       " '▁fais': 828,\n",
       " '▁réseau': 829,\n",
       " 'du': 830,\n",
       " '▁lire': 831,\n",
       " '▁deuxième': 832,\n",
       " '▁jouer': 833,\n",
       " 'ard': 834,\n",
       " '▁met': 835,\n",
       " 'use': 836,\n",
       " '▁22': 837,\n",
       " '▁presque': 838,\n",
       " '▁rue': 839,\n",
       " '▁#': 840,\n",
       " '▁Comment': 841,\n",
       " '▁rôle': 842,\n",
       " '▁auprès': 843,\n",
       " '▁production': 844,\n",
       " 'ré': 845,\n",
       " '▁laisser': 846,\n",
       " '▁table': 847,\n",
       " 'ar': 848,\n",
       " '▁lit': 849,\n",
       " 'ex': 850,\n",
       " '▁enfant': 851,\n",
       " 'amour': 852,\n",
       " '▁mot': 853,\n",
       " '▁bout': 854,\n",
       " '▁cm': 855,\n",
       " 'après': 856,\n",
       " 'ement': 857,\n",
       " '▁cul': 858,\n",
       " '▁couleurs': 859,\n",
       " '▁super': 860,\n",
       " '▁tel': 861,\n",
       " '_': 862,\n",
       " '▁difficile': 863,\n",
       " '▁voiture': 864,\n",
       " '▁parmi': 865,\n",
       " '▁vacances': 866,\n",
       " 'urs': 867,\n",
       " '▁départ': 868,\n",
       " '▁Plus': 869,\n",
       " 'origine': 870,\n",
       " '▁avril': 871,\n",
       " '▁protection': 872,\n",
       " '▁droits': 873,\n",
       " '▁r': 874,\n",
       " '▁blanc': 875,\n",
       " '▁route': 876,\n",
       " '▁Quand': 877,\n",
       " '▁projets': 878,\n",
       " '▁km': 879,\n",
       " 'tte': 880,\n",
       " '▁utiliser': 881,\n",
       " '▁parents': 882,\n",
       " '▁mots': 883,\n",
       " 'isme': 884,\n",
       " '▁nécessaire': 885,\n",
       " '▁milieu': 886,\n",
       " '▁style': 887,\n",
       " '”': 888,\n",
       " '▁Bien': 889,\n",
       " '▁internet': 890,\n",
       " 'to': 891,\n",
       " '▁longtemps': 892,\n",
       " '▁écrit': 893,\n",
       " '▁venir': 894,\n",
       " '▁merci': 895,\n",
       " '▁anti': 896,\n",
       " '▁regard': 897,\n",
       " '▁scène': 898,\n",
       " '▁technique': 899,\n",
       " '▁sait': 900,\n",
       " '▁direction': 901,\n",
       " '▁directement': 902,\n",
       " '▁voix': 903,\n",
       " 'end': 904,\n",
       " 'iste': 905,\n",
       " '▁f': 906,\n",
       " '▁mère': 907,\n",
       " '▁The': 908,\n",
       " '▁pro': 909,\n",
       " '▁devrait': 910,\n",
       " '▁relation': 911,\n",
       " 'os': 912,\n",
       " '▁concernant': 913,\n",
       " 'eux': 914,\n",
       " 'rie': 915,\n",
       " 'ées': 916,\n",
       " '▁avaient': 917,\n",
       " '▁chef': 918,\n",
       " '▁économique': 919,\n",
       " '▁veux': 920,\n",
       " '▁parle': 921,\n",
       " '▁présence': 922,\n",
       " '▁petites': 923,\n",
       " '▁rapide': 924,\n",
       " '▁février': 925,\n",
       " '▁secteur': 926,\n",
       " '▁grandes': 927,\n",
       " '▁commune': 928,\n",
       " '8': 929,\n",
       " '▁texte': 930,\n",
       " '▁unique': 931,\n",
       " '▁solution': 932,\n",
       " '▁St': 933,\n",
       " '▁rouge': 934,\n",
       " '▁durant': 935,\n",
       " '▁)': 936,\n",
       " 'particulièrement': 937,\n",
       " '▁ministre': 938,\n",
       " '▁web': 939,\n",
       " 'um': 940,\n",
       " '▁professionnels': 941,\n",
       " '▁pied': 942,\n",
       " '▁choisir': 943,\n",
       " 'ner': 944,\n",
       " '▁nationale': 945,\n",
       " 'ier': 946,\n",
       " '▁existe': 947,\n",
       " 'ité': 948,\n",
       " '▁sortie': 949,\n",
       " '▁soleil': 950,\n",
       " '▁*': 951,\n",
       " '▁peau': 952,\n",
       " '▁gauche': 953,\n",
       " '▁23': 954,\n",
       " '▁co': 955,\n",
       " '▁état': 956,\n",
       " '▁gamme': 957,\n",
       " 'dé': 958,\n",
       " '7': 959,\n",
       " '▁Conseil': 960,\n",
       " '▁lorsqu': 961,\n",
       " '▁40': 962,\n",
       " '▁contrôle': 963,\n",
       " '▁trouvé': 964,\n",
       " '▁Bon': 965,\n",
       " '▁lequel': 966,\n",
       " '▁pièces': 967,\n",
       " '▁jardin': 968,\n",
       " '▁Ainsi': 969,\n",
       " '▁fil': 970,\n",
       " '▁manque': 971,\n",
       " '▁personnel': 972,\n",
       " '▁réponse': 973,\n",
       " 'action': 974,\n",
       " '▁réaliser': 975,\n",
       " '▁fils': 976,\n",
       " '▁environ': 977,\n",
       " '▁répondre': 978,\n",
       " '▁besoins': 979,\n",
       " 'ton': 980,\n",
       " 'qui': 981,\n",
       " '▁euros': 982,\n",
       " '▁2016': 983,\n",
       " '▁€': 984,\n",
       " '▁plupart': 985,\n",
       " '▁code': 986,\n",
       " '▁aider': 987,\n",
       " '▁DE': 988,\n",
       " '▁celles': 989,\n",
       " 'Le': 990,\n",
       " '▁droite': 991,\n",
       " '▁terrain': 992,\n",
       " '▁conseil': 993,\n",
       " '▁permis': 994,\n",
       " '▁août': 995,\n",
       " '▁via': 996,\n",
       " '▁pourtant': 997,\n",
       " 'ville': 998,\n",
       " '▁changer': 999,\n",
       " '▁certain': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(['test rhume oncologie'], return_tensors='pt', padding=True, truncation=True)\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = set(tokenizer.get_vocab().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'▁lapin' in voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0027472972869873047\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "tokenizer.tokenize(\"Aujourd'hui\")\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens([f'unk{i}' for i in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012057065963745117\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "tokenizer.tokenize(\"Aujourd'hui\")\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey', 'you', 'what', 'are', 'you', 'doing', 'here']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'\\w+', \"Hey, you - what are you doing here!?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(34010, 768)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camembert.resize_token_embeddings(34010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34010"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camembert.get_input_embeddings().num_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0053, -0.0264]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camembert(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -7.7356, -11.9176,  -0.1520,  ...,  -0.5433,   0.1731,  -0.3620],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camembert.get_input_embeddings().weight.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2213e-02, -3.9306e-02, -3.1889e-04,  ..., -1.3756e-03,\n",
       "         -3.9666e-05, -3.3056e-03],\n",
       "        [-1.6021e-02, -2.2969e-02, -1.2812e-02,  ..., -1.3113e-02,\n",
       "         -3.1222e-02, -6.5595e-04],\n",
       "        [-3.2210e-03,  1.8273e-03, -1.8213e-02,  ...,  2.4443e-02,\n",
       "         -8.7075e-04,  2.2326e-02],\n",
       "        [-3.2210e-03,  1.8273e-03, -1.8213e-02,  ...,  2.4443e-02,\n",
       "         -8.7075e-04,  2.2326e-02],\n",
       "        [ 3.0302e-02, -1.6117e-02, -1.0288e-02,  ..., -1.9234e-02,\n",
       "         -4.6506e-04,  8.6418e-03]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor(tokenizer.encode('test simple simple'))\n",
    "camembert.get_input_embeddings()(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0263, -0.0184,  0.0415,  ..., -0.0092, -0.0796,  0.0009],\n",
       "        [-0.0918, -0.0601, -0.2429,  ...,  0.0967, -0.0688, -0.0571],\n",
       "        [ 0.1105,  0.0779, -0.0875,  ..., -0.0956,  0.0994, -0.0822],\n",
       "        [ 0.1105,  0.0779, -0.0875,  ..., -0.0956,  0.0994, -0.0822],\n",
       "        [ 0.0840, -0.0480, -0.0378,  ..., -0.0329, -0.0510,  0.0410]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor(tokenizer.encode('test simple simple'))\n",
    "camembert.get_input_embeddings()(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bos_token',\n",
       " 'eos_token',\n",
       " 'unk_token',\n",
       " 'sep_token',\n",
       " 'pad_token',\n",
       " 'cls_token',\n",
       " 'mask_token',\n",
       " 'additional_special_tokens']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.SPECIAL_TOKENS_ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv('../french_tweets_short.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(df.label)\n",
    "texts = list(df.text)\n",
    "\n",
    "encoding = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n",
    "labels = torch.tensor(labels).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "camembert = CamembertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "optimizer = AdamW(camembert.parameters(), lr=1e-5)\n",
    "classifier = pipeline('sentiment-analysis', model=camembert, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32005, 768, padding_idx=1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camembert(input_ids, attention_mask=attention_mask, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4683, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4551, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4425, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4303, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4185, grad_fn=<NllLossBackward>)\n",
      "tensor(0.4071, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3960, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3850, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3742, grad_fn=<NllLossBackward>)\n",
      "tensor(0.3633, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for _ in range(epochs):\n",
    "    outputs = camembert(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.6743533611297607},\n",
       " {'label': 'LABEL_0', 'score': 0.6857601404190063},\n",
       " {'label': 'LABEL_0', 'score': 0.6726309061050415},\n",
       " {'label': 'LABEL_0', 'score': 0.6749323606491089},\n",
       " {'label': 'LABEL_0', 'score': 0.6603058576583862},\n",
       " {'label': 'LABEL_0', 'score': 0.6522192358970642},\n",
       " {'label': 'LABEL_0', 'score': 0.6360929012298584},\n",
       " {'label': 'LABEL_1', 'score': 0.7538371086120605},\n",
       " {'label': 'LABEL_1', 'score': 0.7436507940292358},\n",
       " {'label': 'LABEL_1', 'score': 0.7174726724624634},\n",
       " {'label': 'LABEL_1', 'score': 0.7531613707542419},\n",
       " {'label': 'LABEL_1', 'score': 0.674791157245636},\n",
       " {'label': 'LABEL_1', 'score': 0.6971939206123352},\n",
       " {'label': 'LABEL_1', 'score': 0.7637169361114502},\n",
       " {'label': 'LABEL_1', 'score': 0.750148355960846},\n",
       " {'label': 'LABEL_1', 'score': 0.7579358816146851}]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = camembert(input_ids, attention_mask=attention_mask, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3852, -0.3428],\n",
       "        [ 0.4054, -0.3750],\n",
       "        [ 0.3783, -0.3418],\n",
       "        [ 0.3592, -0.3714],\n",
       "        [ 0.3408, -0.3239],\n",
       "        [ 0.3279, -0.3009],\n",
       "        [ 0.2998, -0.2586],\n",
       "        [-0.5728,  0.5464],\n",
       "        [-0.5110,  0.5540],\n",
       "        [-0.4302,  0.5017],\n",
       "        [-0.5568,  0.5587],\n",
       "        [-0.3349,  0.3950],\n",
       "        [-0.3959,  0.4381],\n",
       "        [-0.5581,  0.6151],\n",
       "        [-0.5186,  0.5808],\n",
       "        [-0.5655,  0.5758]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(outputs.logits, dim=1).argmax(axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/quentinblampey/cs/French-Transformers'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.sep.join(os.getcwd().split(os.sep)[0 : os.getcwd().split(os.sep).index(\"French-Transformers\") + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
