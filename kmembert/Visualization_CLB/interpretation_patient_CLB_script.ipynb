{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet KmemBERT\n",
    "#### Interprétation patient\n",
    "##### Script pour la visualisation de patient du Centre Léon Bérard\n",
    "\n",
    "Pour exécuter le script, vous aurez besoin :\n",
    "- Si besoin, un jeu de données (format .csv) avec les colonnes suivantes : Noigr, Texte, Date cr, Date deces (si vous souhaitez utiliser des vrais CR issus d'une database).\n",
    "- Du dossier kmembert-base associé au modèle KmemBERT Base.\n",
    "- Du fichier json (large.json) contenant tous les mots du vocabulaire médical, disponible sur le GitHub du projet.\n",
    "\n",
    "Auteur de ce script : Théo Di Piazza (Centre Léon Bérard), avec l'aide des scripts de Mohamed Aymen Qabel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chargement des librairies et création du argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import json\n",
    "import shap\n",
    "from torch.serialization import save\n",
    "from collections import OrderedDict\n",
    "from transformers_interpret import SequenceClassificationExplainer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from kmembert.models import HealthBERT\n",
    "from kmembert.utils import create_session\n",
    "\n",
    "# Import argparse\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-d\", \"--data_folder\", type=str, default=\"data/ehr/test.csv\", \n",
    "    help=\"data path to access to the testing file\")\n",
    "parser.add_argument(\"-p\", \"--path_dataset\", type=str, default=\"data/ehr/test.csv\", \n",
    "    help=\"data path to access to the testing file\")\n",
    "parser.add_argument(\"-r\", \"--resume\", type=str, default=\"kmembert-base\", \n",
    "    help=\"result folder in with the saved checkpoint will be reused\")\n",
    "parser.add_argument(\"-nr\", \"--nrows\", type=int, default=10, \n",
    "    help=\"maximum number of samples for testing\")\n",
    "parser.add_argument(\"-f\", \"--folder_to_save\", type=str, default=\"graphs\", \n",
    "    help=\"folder to save the figures\")\n",
    "parser.add_argument(\"-ng\", \"--noigr\", type=int, default=2, \n",
    "    help=\"The Noigr of a patient\")\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "_, _, device, config = create_session(args)\n",
    "model = HealthBERT(device, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Textes à interpréter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textes à interpréter\n",
    "CR1 = \"Karnofsky = 30 %, PS = 4. Evolutivité de la maladie : Pas de signe d'évolutivité de la maladie.\"\n",
    "CR2 = \"L'état général du patient continue à s'améliorer.\"\n",
    "CR3 = \"Le Karnofsky est actuellement à 30-40 % mais manifestement s'améliore puisque le patient marche désormais.\"\n",
    "CR4 = \"On a une situation de complications multiples (infectieuses, rénales et digestives).\"\n",
    "# id du patient\n",
    "id_noigr = 2022\n",
    "\n",
    "texts_to_classify = np.array([CR1, CR2, CR3, CR4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A partir des CR disponibles, on calcule à partir de chaque compte-rendu l'importance de chaque mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de l'importance et affichage du BarPlot des mots les plus importants\n",
    "\n",
    "# 5 étapes pour afficher le barplot.\n",
    "# ETAPE 1/5 - Texte à classifier : limite de 1500 caractères.\n",
    "# ETAPE 2/5 - Pour chaque Compte-Rendu (CR), calcul de l'importance des tokens.\n",
    "# ETAPE 3/5 - Suppression des tokens doublons. Si tokens récurrent : somme des importances à chaque occurence de ce token.\n",
    "# ETAPE 4/5 - Conserver seulement les mots du vocabulaire médical pour le BarPlot.\n",
    "# ETAPE 5/5 - Affichage du BarPlot et save la figure.\n",
    "\n",
    "\n",
    "# Read medical vocabulary\n",
    "f = open(\"medical_voc/large.json\", encoding='utf-8')\n",
    "dictio = json.load(f)\n",
    "med_voc = []\n",
    "for i in range(len(dictio)):\n",
    "    med_voc.append(dictio[i][0])\n",
    "\n",
    "################################\n",
    "# ETAPE 1/5 - Texte à classifier\n",
    "# On limite la taille de chaque CR à 1500 documents pour éviter qu'ils soient trop long et pas interprétables (Computational Complexity)\n",
    "n_max = 1500\n",
    "# For Loop on text to classify [\"text1..\", ..., \"text4..\"]\n",
    "for i in range(len(texts_to_classify)):\n",
    "    # If length is too long, select only 1500 first caracters\n",
    "    if(len(texts_to_classify[i])>n_max):\n",
    "        n_remove = (len(texts_to_classify[i])-n_max)/2\n",
    "        texts_to_classify[i] = texts_to_classify[i][n_remove:(n_remove+n_max)]\n",
    "\n",
    "################################\n",
    "# ETAPE 2/5 - Pour chaque CR, calcul de l'importance des tokens\n",
    "\n",
    "# Pour chaque CR, on calcule l'importe de chaque token et on conserve les résultats dans word_attributes\n",
    "# word_attributes est une liste de 4 dictionnaire (1 pour chaque CR) de la forme : {token: importance_token, ...}.\n",
    "word_attributes = []\n",
    "for ehr in texts_to_classify:\n",
    "    cls_explainer = SequenceClassificationExplainer(\n",
    "        model.camembert,\n",
    "        model.tokenizer)\n",
    "    word_attributions = cls_explainer(ehr)\n",
    "    word_attributes+= [dict(word_attributions)]\n",
    "\n",
    "################################\n",
    "# ETAPE 3/5 - Suppression des tokens doublons et somme de leur importance\n",
    "# Compute word importance by sum\n",
    "result = {}\n",
    "n_documents = len(word_attributes)\n",
    "for d in word_attributes:\n",
    "    for k in d.keys():\n",
    "        # Si key presente, ajout de la valeur\n",
    "        if k in result:\n",
    "            result[k].append(d[k]/n_documents)\n",
    "        # Sinon, creation de la liste puis ajout de la valeur\n",
    "        else:\n",
    "            result[k] = []\n",
    "            result[k].append(d[k]/n_documents)\n",
    "# Compute standart deviation for each token\n",
    "result_sd = {}\n",
    "for k, v in result.items():\n",
    "    result_sd[k] = np.std(v)\n",
    "\n",
    "################################\n",
    "# ETAPE 4/5 - conserve seulement les mots du vocabulaire médical\n",
    "# Create dict to stock word+attributions for words in medical voc\n",
    "result_medicam = {}\n",
    "result_medicam_sd = {}\n",
    "# For each word of the text to classify\n",
    "for k, v in result.items():\n",
    "    # If special character present, replace it with lower case\n",
    "    new_k = k\n",
    "    if(\"▁\" in k):\n",
    "        new_k = k.replace(\"▁\", \"\").lower()\n",
    "    elif(\"_\" in k):\n",
    "        new_k = k.replace(\"_\", \"\").lower()\n",
    "    # Add word to dict with transformed string, if in medical voc\n",
    "    if new_k in med_voc:\n",
    "        result_medicam[new_k] = v\n",
    "        result_medicam_sd[new_k] = result_sd[k]\n",
    "    \n",
    "# Compute mean of word importance\n",
    "result_mean = {k: np.mean(result_medicam[k]) for k in result_medicam.keys() & result_medicam}\n",
    "\n",
    "################################\n",
    "# ETAPE 5/5 - Affichage du BarPlot et save la figure.\n",
    "\n",
    "# Get 10 most important by mean\n",
    "show_n = 10\n",
    "final_dict = dict(sorted(result_mean.items(), key=lambda item: abs(item[1]), reverse=True)[:show_n])\n",
    "\n",
    "# Get 10 most importants word with values as sum\n",
    "final_dict_sd = dict((k, result_medicam_sd[k]) for k in list(final_dict.keys()))\n",
    "\n",
    "# BARH Plot with everyword - without filter on medical\n",
    "show_n = 10\n",
    "final_dict = dict(sorted(result_mean.items(), key=lambda item: abs(item[1]), reverse=True)[:show_n])\n",
    "################################################################\n",
    "# Plot the results for the word attributes (green for positive contribution and red for negative contribution)\n",
    "x = list(final_dict.keys())\n",
    "y = list(final_dict.values())\n",
    "colors = ['deeppink']*show_n \n",
    "colors = ['deeppink']*len(y) \n",
    "for i in range(len(y)):\n",
    "    if y[i]>0:\n",
    "        colors[i] = 'dodgerblue'\n",
    "fig, ax = plt.subplots()\n",
    "plt.barh(list(reversed(x)), list(reversed(y)), color=list(reversed(colors)))\n",
    "\n",
    "i = show_n-1\n",
    "for k, v in final_dict.items():\n",
    "    if(final_dict_sd[k] != 0):\n",
    "        if(v>=0):\n",
    "            ax.hlines(y=i, xmin=final_dict[k]-final_dict_sd[k], xmax=final_dict[k]+final_dict_sd[k], linewidth=1, colors='black')\n",
    "            ax.vlines(x=final_dict[k]-final_dict_sd[k], ymin=i-0.2, ymax=i+0.2, linewidth=1, color='black')\n",
    "            ax.vlines(x=final_dict[k]+final_dict_sd[k], ymin=i-0.2, ymax=i+0.2, linewidth=1, color='black')\n",
    "        else:\n",
    "            ax.hlines(y=i, xmin=final_dict[k]-final_dict_sd[k], xmax=final_dict[k]+final_dict_sd[k], linewidth=1, colors='black')\n",
    "            ax.vlines(x=final_dict[k]-final_dict_sd[k], ymin=i-0.2, ymax=i+0.2, linewidth=1, color='black')\n",
    "            ax.vlines(x=final_dict[k]+final_dict_sd[k], ymin=i-0.2, ymax=i+0.2, linewidth=1, color='black')\n",
    "    i -= 1\n",
    "\n",
    "plt.title(f'Visualization of the 10 most influential words for the patient {id_noigr}')\n",
    "\n",
    "# Add legend\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "blue_patch = mpatches.Patch(color='dodgerblue')\n",
    "pink_patch = mpatches.Patch(color='deeppink')\n",
    "line_b, = plt.plot([0], [0], color='black')\n",
    "\n",
    "plt.xlabel(\"Importance of the word\")\n",
    "\n",
    "plt.legend([blue_patch, pink_patch, line_b], \n",
    "            ['Good prognosis', 'Bad prognosis', 'Standard deviation'], \n",
    "            numpoints=1, handlelength=3,\n",
    "            loc='lower left',\n",
    "            bbox_to_anchor=(-0.1, -0.275), ncol=3)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(f'graphs/interpretation/test_{id_noigr}_boxplotSD.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Affichage du texte avec les mots surlignés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage du texte avec les mots surlignes\n",
    "\n",
    "# 2 étapes pour afficher le texte surligné:\n",
    "# ETAPE 1/2 - Creation du dictionnaire result_plot avec, pour chaque token, sa valeur d'importance.\n",
    "# ETAPE 2/2 - Mise en forme et affichage du texte.\n",
    "\n",
    "################################\n",
    "# ETAPE 1/2 - Creation du dictionnaire result_plot {token: value}.\n",
    "\n",
    "# Modification du dictionnaire pour afficher les résultats\n",
    "# Création de 'result_plot' pour stocker tous les mots et leur valeur d'attributions\n",
    "result_plot = {}\n",
    "# Pour chaque mot et sa valeur extraite des textes à classifier\n",
    "# k : le token, v : valeur d'importance du token\n",
    "for k, v in result.items():\n",
    "    new_k = k\n",
    "    #  If special character present, replace it and save with lower case\n",
    "    if(\"▁\" in k):\n",
    "        new_k = k.replace(\"▁\", \"\")\n",
    "        result_plot[new_k] = 0\n",
    "    elif(\"_\" in k):\n",
    "        new_k = k.replace(\"_\", \"\")\n",
    "        result_plot[new_k] = 0\n",
    "    result_plot[new_k] = -v[0] # Negative to adjust shap module, idk why...\n",
    "\n",
    "################################\n",
    "# ETAPE 2/2 - Mise en forme et affichage\n",
    "# Mise en forme pour affichage\n",
    "txt = list(result_plot.keys()) # Le texte\n",
    "txt_data = (list(map(( lambda x: x+' '), txt)),)\n",
    "val = list(result_plot.values()) # Les valeurs\n",
    "txt_values = np.array([val])\n",
    "\n",
    "# Création de l'objet shap et affichage\n",
    "test = shap._explanation.Explanation(values=txt_values)\n",
    "test.data = txt_data\n",
    "test.base_values = np.array([0.])\n",
    "shap.plots.text(test[0])\n",
    "################################\n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FIN"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3017c399632bdad99ff7d78fdd8a75c4df8c74c804f4832978fc1a75e7107017"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('clb_env2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
